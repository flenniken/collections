#!/usr/bin/python3

# This is the help text shown with -h:
"""
Deploy the modified website files to s3 and invalidate them in the
cloudfront cache.
"""

import os

if not os.environ.get("coder_env"):
  print("Run from the Collection's docker environment.")
  exit(1)

import sys
import argparse
import traceback
import sys
import signal
import unittest
import json
import subprocess
import sys
import textwrap
import boto3
from datetime import datetime

class DeployException(Exception):
  """ An exception we plan for. """
  pass

def getAwsSettings():
  """ Get the aws settings from the env/aws-settings.json file. """
  with open('/home/coder/collections/env/aws-settings.json', 'r') as file:
    return json.load(file)

def listS3(s3Client, bucket_name):
  """ List the first 1000 S3 files in the collection's bucket. """

  fileInfo = getFileInfo(s3Client, bucket_name)
  for info in fileInfo:
    timeStr = str(info['LastModified'])[0:-6]
    print(f"{timeStr} {info['Size']:8.0f} {info['Key']}")

def getFileInfo(s3Client, bucket_name):
  """ Get information about the first 1000 files in the given bucket. """

  # Python boto3 docs for S3:
  # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html

  response = s3Client.list_objects_v2(Bucket=bucket_name)
  # print(json.dumps(response, default=str))
  # "Contents": [
  #   {
  #     "Key": "collections.css",
  #     "LastModified": "2024-11-23 20:19:33+00:00",
  #     "ETag": "\"9a0da3be7efd0bff1fb5e94874eb5f16\"",
  #     "Size": 2970,
  #     "StorageClass": "STANDARD"
  #   },
  fileInfo = []
  for info in response.get("Contents"):
    fileInfo.append(info)
  return fileInfo

def syncModifiedFiles(s3Client, bucket_name, cloudfrontClient, distribution_id):
  """
  Copy the modified website files to S3 and invalidate them in the
  cloudfront cache. Print the S3 files invalidated.
  """
  modifiedFiles = getModifiedFiles(s3Client, bucket_name, sync=True)

  # Invalidate and print the modified files on S3. Strip off the
  # leading "dist" from the modified files to match the location on
  # S3.
  paths = []
  for filename in modifiedFiles:
    assert(filename.startswith("dist"))
    # Remove the leading "dist", leaving /index.htm for example.
    path = filename[4:]
    paths.append(path)
    print(path)

  invalidateCloudFrontCache(cloudfrontClient, distribution_id, paths)

def invalidateCloudFrontCache(cloudfrontClient, distribution_id, paths):
  """
  Invalidate the cloudfront distribution cache for the given list
  of filenames.
  """
  if not paths:
    return

  # Invalidate a list of filenames on S3.
  response = cloudfrontClient.create_invalidation(
    DistributionId=distribution_id,
    InvalidationBatch={
      'Paths': {
        'Quantity': len(paths),
        'Items': paths
      },
      'CallerReference': str(datetime.now().timestamp())
    }
  )

def listModified(s3Client, bucket_name):
  """
  List the modified website files.
  """
  modifiedFiles = getModifiedFiles(s3Client, bucket_name, sync=False)
  for filename in modifiedFiles:
    print(filename)

def getModifiedFiles(s3Client, bucket_name, sync=False):
  """
  Get the modified website files in the dist folder and optionally sync them to S3.
  """
  # sample output from the sync command:
  # (dryrun) upload: dist/index.html to s3://sflennikco/index.html
  # (dryrun) upload: dist/js/image.js to s3://sflennikco/js/image.js

  if sync:
    command = f"aws s3 sync --no-progress dist s3://{bucket_name}" + " | awk '{print $2}'"
  else:
    command = f"aws s3 sync --no-progress --dryrun dist s3://{bucket_name}" + " | awk '{print $3}'"
  # print(command)
  result = subprocess.run(command, shell=True, capture_output=True, text=True)
  text = result.stdout

  # Return a list of the modified filenames.
  filenames = []
  lines = text.split('\n')
  for line in lines:
    if line:
      filenames.append(line)
  return filenames

# todo: share this code
def getBotoClient(serviceName='s3'):
  """
  Return the boto3 client for the given AWS service. Services names
  like 'cognito-idp', 'ses', 'sts', etc.
  """
  configFilename = "/home/coder/.aws/config"
  credsFilename = "/home/coder/.aws/credentials"
  if not os.path.exists(configFilename) or not os.path.exists(credsFilename):
     raise DeployException("""\

Before this script can access your aws s3 bucket, you need an
AWS account and an IAM user with with the correct permissions.
See the docs for how to setup the IAM user.
""")
  return boto3.client(serviceName)

def process(args):
  """ Handle the command specified by the user. """

  if args.sync or args.listS3 or args.listModified:
    awsSettings = getAwsSettings()
    settings = awsSettings['settings']
    s3Client = getBotoClient('s3')
    bucket_name = settings['bucket_name']
    if args.listS3:
      listS3(s3Client, bucket_name)
      return
    elif args.sync:
      cloudfrontClient = getBotoClient('cloudfront')
      syncModifiedFiles(s3Client, bucket_name, cloudfrontClient,
        settings['distribution_id'])
      return
    elif args.listModified:
      listModified(s3Client, bucket_name)
      return

  raise DeployException("Invalid argument")

def parseCommandLine():
  """
  Parse the command line arguments.
  """
  if len(sys.argv) == 1:
    # When there are no arguments provided, show the help information.
    sys.argv.append("-h")
  else:
    # When -t is provided, change argv so unittest does not see it.
    # You can run one test by providing the test name as an argument e.g.:
    # scripts/deploy -t TestModule.test_me
    for ix, arg in enumerate(sys.argv[1:]):
      if arg in ['-t', '--test']:
        sys.argv.pop(ix+1)
        args = argparse.Namespace(test = True)
        return args

  # Use this module's comment at the top of the file for the
  # description.
  parser = argparse.ArgumentParser(
  prog='PROG', formatter_class=argparse.RawDescriptionHelpFormatter,
  description=textwrap.dedent(__doc__))

  parser.add_argument("-t", "--test", action="store_true",
    help="""run all or one internal unit tests. You can run just one like this:
    scripts/deploy -t TestModule.test_me""")

  parser.add_argument("-l", "--listS3", action='store_true',
    help="list the first 1,000 collections files in S3 ")

  parser.add_argument("-s", "--sync", action='store_true',
    help="copy the modified files to S3 and invalidate them in the cloudfront cache")

  parser.add_argument("-m", "--listModified", action='store_true',
    help="list the modified files")

  args = parser.parse_args(sys.argv[1:])
  return args

def signalHandler(sig, frame):
  """ Handle ctrl-C """
  print("\n")
  sys.exit(1)

def main(args):
  signal.signal(signal.SIGINT, signalHandler)
  try:
    process(args)
  except KeyboardInterrupt:
    print("\nStopping")
  except DeployException as ex:
    print(str(ex))
  except Exception as ex:
    print("Unexpected exception")
    print(traceback.format_exc())
    return 1
  return 0 # success

class TestClients:
  def __init__(self):
    self.s3Client = getBotoClient('s3')
    self.sesClient = getBotoClient('cloudfront')

tc = None

def setUpModule():
  # Get the boto clients we test. This method is called once by the
  # test suite. The code is here to avoid a runtime error in the
  # shutdown phase.
  global tc
  tc = TestClients()

def tearDownModule():
  pass

class TestModule(unittest.TestCase):

  def test_me(self):
    self.assertTrue(1)

  def test_getAwsSettings(self):
    awsSettings = getAwsSettings()
    self.assertTrue(awsSettings is not None)
    self.assertTrue('settings' in awsSettings)
    settings = awsSettings['settings']
    # text = json.dumps(settings, indent=2, default=str)
    # print(f"settings: {text}")

    # [aws_settings.json]
    keys = ["client_id", "redirect_uri", "logout_uri", "scope", "domain", "pool_name", "distribution_id", "bucket_name", "userPoolId"]
    for key in keys:
      if key not in settings:
        print(f"missing key: {key}")
      self.assertTrue(key in settings)
    self.assertEqual(len(settings['client_id']), 26)
    self.assertTrue(settings['redirect_uri'].startswith("https://"))
    self.assertTrue(settings['redirect_uri'].endswith("/index.html"))
    self.assertTrue(settings['logout_uri'].startswith("https://"))
    self.assertTrue(settings['logout_uri'].endswith("/index.html"))
    self.assertEqual(settings['scope'], "openid profile aws.cognito.signin.user.admin")
    self.assertTrue(settings['domain'].startswith("https://pool"))
    self.assertTrue(settings['domain'].endswith(".amazoncognito.com"))
    self.assertEqual(settings['pool_name'], "collections-pool")
    self.assertEqual(len(settings['distribution_id']), 13)

  def test_getFileInfo(self):
    awsSettings = getAwsSettings()
    settings = awsSettings['settings']
    result = getFileInfo(tc.s3Client, settings['bucket_name'])
    # print(json.dumps(result, indent=2, default=str))
    self.assertTrue(len(result) <= 1000)
    for info in result:
      # print(info['Key'])
      assert('Key' in info)
      assert('LastModified' in info)
      assert('Size' in info)
      assert('StorageClass' in info)
      assert('ETag' in info)
      self.assertEqual(info['StorageClass'], "STANDARD")

  def test_getModifiedFiles(self):
    awsSettings = getAwsSettings()
    settings = awsSettings['settings']
    modifiedFiles = getModifiedFiles(tc.s3Client, settings['bucket_name'], sync=True)
    # print(f"modifiedFiles: {modifiedFiles}")

if __name__ == '__main__':
  args = parseCommandLine()
  if args.test:
    sys.exit(unittest.main())
  rc = main(args)
  sys.exit(rc)
