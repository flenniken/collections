#!/usr/bin/python3

# This is the help text shown with -h:
"""
Deploy the modified website files to s3 and invalidate them in the
cloudfront cache.
"""

import os

if not os.environ.get("coder_env"):
  print("Run from the Collection's docker environment.")
  exit(1)

import sys
import argparse
import traceback
import signal
import unittest
import json
import subprocess
import textwrap
import boto3
from datetime import datetime
from boto_client import getBotoClient, getAwsSettings
import time
import hashlib
import mimetypes

class DeployException(Exception):
  """ An exception we plan for. """
  pass

def getFileInfo(s3Client, bucket_name, exclude_prefix):
    """
    Get information about the files in the given bucket with the
    given prefix. Supports pagination and filters out items that
    start with the exclude_prefix.
    """
    fileInfo = []
    continuation_token = None

    while True:
      # Prepare the request parameters
      request_params = {"Bucket": bucket_name}
      if continuation_token:
        request_params["ContinuationToken"] = continuation_token

      # Fetch the response
      response = s3Client.list_objects_v2(**request_params)

      # Process the response
      for info in response.get("Contents", []):
        key = info["Key"]
        # Exclude items that start with the exclude_prefix.
        if not key.startswith(exclude_prefix):
          fileInfo.append(info)

      # Check if there are more pages
      if response.get("IsTruncated"):
        continuation_token = response["NextContinuationToken"]
      else:
        break

    return fileInfo

def copyFilesToS3(s3Client, bucket_name, filenames):
  """
  Copy the given list of filenames to the given S3 bucket.
  The filenames are relative to the dist folder.
  """
  if not filenames:
    return

  for filename in filenames:
    # Ensure the filename does not start with a forward slash
    assert(not filename.startswith("/"))

    # Construct the local path in the dist folder
    localPath = os.path.join("dist", filename)

    content_type, _ = mimetypes.guess_type(localPath)
    if content_type is None:
      content_type = 'binary/octet-stream'

    # Upload the file to S3
    print(f"Copying {localPath} to s3 as {content_type}")
    s3Client.upload_file(localPath, bucket_name, filename,
      ExtraArgs={'ContentType': content_type})

def syncModifiedFiles(s3Client, bucket_name, cloudfrontClient, distribution_id):
  """
  Copy the modified website files to S3 and invalidate them in the CloudFront cache.
  Print the S3 files invalidated.
  """
  modifiedFiles = getModifiedFiles(s3Client, bucket_name)
  if not modifiedFiles:
    print("No modified files found.")
    return

  # Copy the modified files to S3.
  copyFilesToS3(s3Client, bucket_name, modifiedFiles)

  # Invalidate the modified files in the CloudFront cache.
  invalidation_id = invalidateCloudFrontCache(cloudfrontClient, distribution_id, modifiedFiles)

  # Wait for the CloudFront invalidation to complete.
  if invalidation_id:
    waitForDeployed(cloudfrontClient, distribution_id, invalidation_id)

def invalidateCloudFrontCache(cloudfrontClient, distribution_id, paths):
  """
  Invalidate the CloudFront distribution cache for the given list
  of filenames. Return the invalidation ID.
  """
  if not paths:
    return None

  # Ensure all paths start with a forward slash
  normalized_paths = [f"/{path}" if not path.startswith("/") else path for path in paths]

  response = cloudfrontClient.create_invalidation(
    DistributionId=distribution_id,
    InvalidationBatch={
      'Paths': {
          'Quantity': len(normalized_paths),
          'Items': normalized_paths
      },
      'CallerReference': str(datetime.now().timestamp())
    }
  )
  return response['Invalidation']['Id']

def waitForDeployed(cloudfrontClient, distribution_id, invalidation_id):
  """
  Wait for the CloudFront invalidation to complete.
  """
  print("Waiting for the CloudFront invalidation to complete...")
  count = 0
  wait_seconds = 5

  while True:
    response = cloudfrontClient.get_invalidation(
      DistributionId=distribution_id, Id=invalidation_id
    )
    invalidation_status = response['Invalidation']['Status']
    # Print the status for debugging
    print(f"status: {invalidation_status}")

    if invalidation_status == "Completed":
      print(f"Invalidation completed, waited: {count * wait_seconds} seconds")
      break

    count += 1
    sys.stdout.flush()
    time.sleep(wait_seconds)

def listModified(s3Client, bucket_name):
  """
  List the modified website files.
  """
  modifiedFiles = getModifiedFiles(s3Client, bucket_name)
  for filename in modifiedFiles:
    print(filename)

def getFileHashes(path):
  """
  Get the md5 hashes of the files in the given folder.
  """
  fileHashes = {}
  for root, dirs, files in os.walk('dist'):
    for filename in files:
      filepath = os.path.join(root, filename)
      if not os.path.isfile(filepath):
        continue
      with open(filepath, 'rb') as f:
        file_content = f.read()
        md5_hash = hashlib.md5(file_content).hexdigest()
        # Store the relative path to the file in the dist folder.
        relative_path = os.path.relpath(filepath, 'dist')
        fileHashes[relative_path] = md5_hash
  return fileHashes

def getS3FileHashes(s3Client, bucket_name):
  """
  Get the md5 hashes of the files in the given S3 bucket excluding
  the logs/ entries.
  """
  fileInfo = getFileInfo(s3Client, bucket_name, "logs/")
  s3FileHashes = {}
  for info in fileInfo:
    filename = info['Key']
    # The ETag is the md5 hash of the file content.
    md5_hash = info['ETag'].strip('"')
    s3FileHashes[filename] = md5_hash
  return s3FileHashes

def getModifiedFiles(s3Client, bucket_name):
  """
  Return the modified website files in the dist folder.
  """
  modified = []
  # Get the dist files and their md5 hashes. Return a dictionary mapping
  # the filename to the md5 hash.
  s3FileHashes = getS3FileHashes(s3Client, bucket_name)
  distFileHashes = getFileHashes("dist")

  for filename, md5 in distFileHashes.items():
    s3hash = s3FileHashes.get(filename, None)
    if s3hash is None:
      modified.append(filename)
      continue
    if s3hash != md5:
      modified.append(filename)

  return modified

def process(args):
  """ Handle the command specified by the user. """

  awsSettings = getAwsSettings()
  settings = awsSettings['settings']
  bucket_name = settings['bucket_name']
  distribution_id = settings['distribution_id']

  if args.sync:
    s3Client = getBotoClient('s3')
    cloudfrontClient = getBotoClient('cloudfront')
    syncModifiedFiles(s3Client, bucket_name, cloudfrontClient,
      distribution_id)
  elif args.listModified:
    s3Client = getBotoClient('s3')
    listModified(s3Client, bucket_name)
  else:
    raise DeployException("Invalid argument")

def parseCommandLine():
  """
  Parse the command line arguments.
  """
  if len(sys.argv) == 1:
    # When there are no arguments provided, show the help information.
    sys.argv.append("-h")
  else:
    # When -t is provided, change argv so unittest does not see it.
    # You can run one test by providing the test name as an argument e.g.:
    # scripts/deploy -t TestModule.test_me
    for ix, arg in enumerate(sys.argv[1:]):
      if arg in ['-t', '--test']:
        sys.argv.pop(ix+1)
        args = argparse.Namespace(test = True)
        return args

  # Use this module's comment at the top of the file for the
  # description.
  parser = argparse.ArgumentParser(
  prog='PROG', formatter_class=argparse.RawDescriptionHelpFormatter,
  description=textwrap.dedent(__doc__))

  parser.add_argument("-t", "--test", action="store_true",
    help="""run all or one internal unit tests. You can run just one like this:
    scripts/deploy -t TestModule.test_me""")

  parser.add_argument("-s", "--sync", action='store_true',
    help="copy the modified files to S3 and invalidate them in the cloudfront cache")

  parser.add_argument("-m", "--listModified", action='store_true',
    help="list the modified files")

  args = parser.parse_args(sys.argv[1:])
  return args

def signalHandler(sig, frame):
  """ Handle ctrl-C """
  print("\n")
  sys.exit(1)

def main(args):
  signal.signal(signal.SIGINT, signalHandler)
  try:
    process(args)
  except KeyboardInterrupt:
    print("\nStopping")
  except DeployException as ex:
    print(str(ex))
  except Exception as ex:
    print("Unexpected exception")
    print(traceback.format_exc())
    return 1
  return 0 # success

class TestClients:
  def __init__(self):
    self.s3Client = getBotoClient('s3')
    self.sesClient = getBotoClient('cloudfront')

tc = None

def setUpModule():
  # Get the boto clients we test. This method is called once by the
  # test suite. The code is here to avoid a runtime error in the
  # shutdown phase.
  global tc
  tc = TestClients()

def tearDownModule():
  pass

class TestModule(unittest.TestCase):

  def test_me(self):
    self.assertTrue(1)

  def test_getAwsSettings(self):
    awsSettings = getAwsSettings()
    self.assertTrue(awsSettings is not None)
    self.assertTrue('settings' in awsSettings)
    settings = awsSettings['settings']
    # text = json.dumps(settings, indent=2, default=str)
    # print(f"settings: {text}")

    # [aws_settings.json]
    keys = ["client_id", "redirect_uri", "logout_uri", "scope", "domain", "pool_name", "distribution_id", "bucket_name", "userPoolId"]
    for key in keys:
      if key not in settings:
        print(f"missing key: {key}")
      self.assertTrue(key in settings)
    self.assertEqual(len(settings['client_id']), 26)
    self.assertTrue(settings['redirect_uri'].startswith("https://"))
    self.assertTrue(settings['redirect_uri'].endswith("/index.html"))
    self.assertTrue(settings['logout_uri'].startswith("https://"))
    self.assertTrue(settings['logout_uri'].endswith("/index.html"))
    self.assertEqual(settings['scope'], "openid profile aws.cognito.signin.user.admin")
    self.assertTrue(settings['domain'].startswith("https://pool"))
    self.assertTrue(settings['domain'].endswith(".amazoncognito.com"))
    self.assertEqual(settings['pool_name'], "collections-pool")
    self.assertEqual(len(settings['distribution_id']), 13)

  def test_getFileInfo(self):
    awsSettings = getAwsSettings()
    settings = awsSettings['settings']
    fileinfo = getFileInfo(tc.s3Client, settings['bucket_name'], 'logs/')
    for info in fileinfo:
      # print(info['Key'])
      assert('Key' in info)
      assert('LastModified' in info)
      assert('Size' in info)
      assert('StorageClass' in info)
      assert('ETag' in info)
      self.assertEqual(info['StorageClass'], "STANDARD")
      self.assertTrue(not info['Key'].startswith("logs/"))
      self.assertTrue(len(info['Key']) > 0)
      self.assertTrue(info["Size"] >= 0)

  def test_getFileHashes(self):
    fileHashes = getFileHashes('dist')
    # print("")
    # for filename, md5 in fileHashes.items():
    #   print(f"{md5}: {filename}")
    self.assertTrue(len(fileHashes) > 0)
    for filename, md5 in fileHashes.items():
      self.assertTrue(len(filename) > 0)
      self.assertEqual(len(md5), 32)
      self.assertTrue(md5.isalnum())

  def test_getS3FileHashes(self):
    awsSettings = getAwsSettings()
    settings = awsSettings['settings']
    fileHashes = getS3FileHashes(tc.s3Client, settings['bucket_name'])
    # print("")
    # for filename, md5 in fileHashes.items():
    #   print(f"{md5}: {filename}")
    self.assertTrue(len(fileHashes) > 0)
    for filename, md5 in fileHashes.items():
      self.assertTrue(len(filename) > 0)
      self.assertEqual(len(md5), 32)
      self.assertTrue(md5.isalnum())

if __name__ == '__main__':
  args = parseCommandLine()
  if args.test:
    sys.exit(unittest.main())
  rc = main(args)
  sys.exit(rc)
